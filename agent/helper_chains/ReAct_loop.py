import sys
if "../" not in sys.path:
    sys.path.append("../")
if "../../" not in sys.path:
    sys.path.append("../../")
if "../../../" not in sys.path:
    sys.path.append("../../../")

from utils.initialize_gemini import initialize_gemini
from prompts.ReAct_system_template import ReAct_system_template
from langchain_core.prompts import ChatPromptTemplate

#from helper_chains.vectorstore import vectorstore_chain
from helper_chains.web_search import web_search_chain
from helper_chains.llm_knowledge import llm_knowledge_chain

from Allam_Features.to_MSA.chain import ToMSAChain 
from Allam_Features.to_MSA_paragraph.chain import ToMSAParagraphChain
from Allam_Features.irab.chain import IrabProcessor
from Allam_Features.tashkeel.chain import TashkeelChain

import re
from termcolor import colored
class ReActLoop:
    """
    ReActLoop class implements the ReAct loop prompting-technique for processing queries.

    The ReActLoop class manages the interaction between a large language model (LLM)
    and external tools to provide comprehensive and informative responses to user queries.
    """
    general_verbosity = True
    def __init__(self, verbose):
        """
        Initializes the ReActLoop object.

        Args:
            verbose (bool, optional): Whether to print verbose output. Defaults to True.
        """
        ReActLoop.general_verbosity = verbose
        self.system = ReAct_system_template
        self.ReAct_model = initialize_gemini(api_config_path="config/api1.yaml") 
        self.messages = []        
        if self.system:
            self.messages.append(("system", self.system))

    def _execute(self, message:str=""):
        """
        Executes one message through the ReAct model.
        This method will be called many times within the main ReAct loop.
        Args:
            message (str, optional): The input message. Defaults to "".

        Returns:
            str: The generated response from the LLM.
        """
        verbose = ReActLoop.general_verbosity

        if message:
            self.messages.append(("user",message))
        result = self.ReAct_model.invoke(ChatPromptTemplate(self.messages).invoke({}))
        if verbose:
            print(colored(message,"cyan"))
        self.messages.append(("ai",result.content))
        return result.content

    
    def loop(self, Query):
        """
        Runs the main ReAct loop to process a query.

        Args:
            Query (str): The input query.

        Returns:
            str: The final answer generated by the ReAct loop.
        """
        verbose=ReActLoop.general_verbosity

        # define the available tools to be used by ReAct loop actions.
        
        # to_MSA = ToMSAChain(cares_about_requests=False)
        to_MSA =  ToMSAParagraphChain(chunk_size=50)
        irab = IrabProcessor()
        tashkeel = TashkeelChain()
        web_search =  web_search_chain()
        llm_knowledge = llm_knowledge_chain()
        
        # Execute the initial prompt
        response = self._execute(Query)
        print(colored(response,"cyan")) if verbose else None
        tool = 'None'

        # Main ReAct loop
        for i in range(20):
            # Check for PAUSE in the output and extract the tool and tool input returend from (Action:)
            if "PAUSE" in response and "answer:" not in response.lower():
                match = re.findall(r'Action: (to_MSA|irab|tashkeel|web_search|llm_knowledge): "(.*?)"', response)
                if match:
                    tool, tool_input = match[0][0], match[0][1]
                
                # Execute the tool and update the response to be given back to the LLM to continue ReAct loop
                if tool != 'None':
                    tool_res = eval(f"{tool}(\"{tool_input}\")")
                    response = f"Observation: {tool_res}"
                else:
                    response = "Observation: I cannot answer using my available actions. so I will inform the user to explain more its query as I cannot answer it"
            
            # Extract the final answer (Answer:)from the response if the ReAct loop reached its end by the LLM
            elif "answer:" in response.lower():
                if tool == 'to_MSA':
                    tool = tool+" ðŸ•µï¸" 
                elif tool == 'irab':
                    tool = tool+" ðŸ“š" 
                elif tool == 'tashkeel':
                    tool = tool+" ðŸ‡µðŸ‡¸" 
                elif tool == 'web_search':
                    tool = tool+" ðŸŒ"
                elif tool == 'llm_knowledge':
                    tool = tool+" ðŸ¤–" 
                
                try:
                    response = f"From {tool} >> "+ re.findall("Answer: .*", response, re.DOTALL)[0] if tool != 'None' else re.findall("Answer: .*", response, re.DOTALL)[0]
                except Exception as e:
                    try:
                        response = f"From {tool} --> "+ re.findall(r"Thought: (.+)", response, re.IGNORECASE)[0] if tool != 'None' else re.findall(r"Thought: (.+)", response, re.IGNORECASE)[0]
                    except Exception as e:
                        print("---- while giving asnwer, we recieved this ---", e)
                        response = "Answer: Please ask me again as There is an Error displaying the results."
                return response
            
            # Send the response (given from above) to the LLM to continue its ReAct loop.
            response = self._execute(response)
            print(colored(response,"cyan")) if verbose else None
        return response
    
    def __call__(self, Query):
        """
        exectures the ReAct loop when the class object like this `object(query)` is called instead of running `object.loop(query)`.
        """
        return self.loop(Query)