import sys, os

# Add the parent directory
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from agent_prompts.llm_knowledge_prompt import prompt
from langchain_google_genai import ChatGoogleGenerativeAI

from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

class LLMKnowledgeChain:
    """
    This class leverages a large language model (LLM) to responed to user queries 
    that cannot be answered from web_search or from vectorstore.
    """
    def __init__(self, instance):
        self.instance = instance

    def __call__(self, query):
        """
        Processes a query using an LLM knowledge chain.

        Args:
            query (str): The input query.

        Returns:
            str: The answer generated by the LLM based on its knowledge.
        """
        # Load conversation history (if any) for chat-memory purposes
        parent_directory = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        data_file_path = os.path.join(parent_directory, "agent_memory", f"memory_for_user_{self.instance.id}.txt")
        with open(data_file_path,"r") as f:
            current_memory = f.read()

        gemini_llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", api_key=self.instance.gemini_keys[self.instance.iterator % len(self.instance.gemini_keys)])
                
        # Build the LLM knowledge chain
        chain = (
                {"question": RunnablePassthrough(), "history":  RunnableLambda(lambda x: current_memory)}
                | prompt
                | gemini_llm
                | JsonOutputParser()
        )
        # Execute the chain and return the answer
        result = chain.invoke(query)['answer'] 
        self.instance.iterator += 1

        return result 
        